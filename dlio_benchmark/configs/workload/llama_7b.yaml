# 8 node run with 4 GPUs per node and TPSIZE=4 and PPSIZE=8
model:
  name: llama_7b
  type: transformer
  num_layers: 32
  model_datatype: fp16
  optimizer_datatype: fp32
  parallelism: 
    pipeline: 1
    tensor: 1
    zero_stage: 1
  transformer: 
    vocab_size: 32000
    hidden_size: 4096
    ffn_hidden_size: 11008
    num_attention_heads: 32
    num_kv_heads: 32

framework: pytorch

workflow:
  generate_data: False
  train: False
  checkpoint: True

checkpoint:
  checkpoint_folder: checkpoints/llama_7b
  time_between_checkpoints: 5
  num_checkpoints_write: 10
  num_checkpoints_read: 10
  fsync: True
